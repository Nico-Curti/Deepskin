{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6678e12-e9c9-4628-badf-059a0f2525e0",
   "metadata": {},
   "source": [
    "# Active Semi-Supervised Learning training strategy\n",
    "\n",
    "In this demo we will show how to perform an active semi-supervised learning strategy for the training of a deep learning neural network which aims to perform the wound segmentation on smartphone images.\n",
    "In this demo we will set generic folders for the images and related masks, which mimic as much as possible the original Deepskin dataset.\n",
    "Thus, the proposed code is ready-to-use with your custom dataset and it could be easily adapted also for other segmentation purposes.\n",
    "\n",
    "The model used for the image segmentation is a classical EfficientNet-b3 architecture, which is already implemented in the `deepskin` Python package.\n",
    "The detailed description about the model architecture could be found in the work of Curti et al. [1](https://www.mdpi.com/1422-0067/24/1/706).\n",
    "\n",
    "We used Tensorflow library for the model implementation, so be sure to have installed all the required package before the use of this script.\n",
    "\n",
    "First of all, we need to import the required libraries and define the common variables and path for the use of the segmentation model.\n",
    "The below script could be used for **all** the ASSL rounds of training, with minimum chaning in the below global variables.\n",
    "Take care about the code comments for the correct usage of the script in different rounds.\n",
    "You must take care about the folder tree in which the data (images and masks) are stored.\n",
    "In this demo we assume a folder tree described as:\n",
    "\n",
    "```bash\n",
    "data/\n",
    "├── deepskin_images_round0\n",
    "├── deepskin_masks_round0\n",
    "├── deepskin_images\n",
    "├── deepskin_masks\n",
    "├── validation_images_pred_round0\n",
    "└── validation_masks_pred_round0\n",
    "```\n",
    "\n",
    "where:\n",
    "* `deepskin_images` contains the entire set of available images in the dataset;\n",
    "* `deepskin_masks` contains the entire set of **validated** masks in the dataset;\n",
    "* `deepskin_images_round0` contains the images to use during the current (round 0) round of ASSL training;\n",
    "* `deepskin_masks_round0` contains the masks to use during the current (round 0) round of ASSL training;\n",
    "* `validation_images_pred_round0` will be filled by the images with the predictions overlayed for the ASSL validation;\n",
    "* `validation_masks_pred_round0` will be filled by the predictions of the trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2262311-53e2-46db-ab49-c38205c88c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# define the current round number\n",
    "ASSL_ROUND = 0\n",
    "# define the batch-size to use during the training\n",
    "BATCH = 8\n",
    "# define the directory in which the whole DB of images are stored\n",
    "ALL_IMAGE_FOLDER = './data/deepskin_images'\n",
    "# define the directory in which the whole DB of (validated!!) masks are stored\n",
    "ALL_MASKS_FOLDER = './data/deepskin_masks'\n",
    "# define the directory in which the images are stored\n",
    "TRAIN_IMAGE_FOLDER = f'./data/deepskin_images_round{ASSL_ROUND:d}'\n",
    "# define the directory in which the masks are stored\n",
    "TRAIN_MASKS_FOLDER = f'./data/deepskin_masks_round{ASSL_ROUND:d}'\n",
    "# define the directory in which the predictions will be saved for the validation\n",
    "PRED_IMAGE_FOLDER = f'./data/validation_images_pred_round{ASSL_ROUND:d}'\n",
    "# define the directory in which the predictions will be saved\n",
    "PRED_MASKS_FOLDER = f'./data/validation_masks_pred_round{ASSL_ROUND:d}'\n",
    "\n",
    "# crete the prediction folder if needed\n",
    "os.makedirs(PRED_IMAGE_FOLDER, exist_ok=True)\n",
    "os.makedirs(PRED_MASKS_FOLDER, exist_ok=True)\n",
    "\n",
    "# define the output weight file for the best model checkpint\n",
    "OUT_WEIGHT_FILE = f'./checkpoints/model_round{ASSL_ROUND:d}.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462f9ee7-d07f-49c8-a90f-4a5f0e3d1fed",
   "metadata": {},
   "source": [
    "To monitor also the development of the ASSL strategy, we can take a look at the statistics related to the considered data against the totality.\n",
    "In each ASSL round we will split the available round-data into a training set (90%) and a validation set (10%): the training set will be used for the tuning of the model parameters, while the remaining validation set will be used for the monitoring of the model performances in a set of independent images.\n",
    "For sake of completeness, we will check all the relevant stats for the round evaluation with the following log:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9739c75e-1c93-4c9b-a4c7-f153cd1ac982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All available images: 6225\n",
      "Images used at round 0: 4936 (79.293%)\n",
      "Images used as training at round 0: 4442 (90%)\n",
      "Images used as test at round 0: 494 (10%)\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "all_files = glob(f'{ALL_IMAGE_FOLDER}/*')\n",
    "print(f'All available images: {len(all_files):d}')\n",
    "round_files = glob(f'{TRAIN_IMAGE_FOLDER}/*')\n",
    "round_perc = len(round_files)/len(all_files)*100\n",
    "print(f'Images used at round {ASSL_ROUND:d}: {len(round_files):d} ({round_perc:.3f}%)')\n",
    "train_perc = round(len(round_files)*.9)\n",
    "test_perc = round(len(round_files)*.1)\n",
    "print(f'Images used as training at round {ASSL_ROUND:d}: {train_perc:d} (90%)')\n",
    "print(f'Images used as test at round {ASSL_ROUND:d}: {test_perc:d} (10%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebd1a9d-d13c-4bc1-8731-319bee8d6e57",
   "metadata": {},
   "source": [
    "Since we want to perform also a data-augmentation step during the training, we will use the APIs of the Tensorflow library for the correct management of the images and desired transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948223f9-3bee-492a-ba5d-2c005877d61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# define the data-augmentation parameters\n",
    "augmentation_params = {\n",
    "    'rotation_range':360,       # all possible rotations\n",
    "    'width_shift_range':0.0,    # avoid width shift\n",
    "    'height_shift_range':0.0,   # avoid height shift\n",
    "    'fill_mode':'reflect',      # use reflection to fill the augmented image\n",
    "    'shear_range':0.,           # avoid shear\n",
    "    'zoom_range':0.,            # avoid zoom\n",
    "    'horizontal_flip':True,     # perform horizontal flip of the image\n",
    "    'vertical_flip':True,       # perform vertical flip of the image\n",
    "    'cval':0.,                  # just the constant value for the augmented background\n",
    "    'validation_split':0.1,     # set the validation set as the 10% of the entire set of data\n",
    "}\n",
    "\n",
    "# define the data augmentation models for images and masks\n",
    "# NOTE: both images and masks must be rescaled into [0, 1] range for \n",
    "# the correct use of the segmentation model!\n",
    "image_augmentation = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    **augmentation_params,\n",
    "    rescale = 1./255\n",
    ")\n",
    "masks_augmentation = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    **augmentation_params,\n",
    "    rescale = 1./255\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5ada42-a193-4d89-9f22-c247b747a6c6",
   "metadata": {},
   "source": [
    "Now we need to define the data-loader strategy for the images/masks.\n",
    "Using the global information about the folder tree and the data-augmentation models, we can use the Tensorflow APIs as follow.\n",
    "\n",
    "**NOTE**: In the following snippet we assume a model for a **semantic classification task** (new version of the deepskin segmentation model), setting the `color_mode` of the mask files as `rgb`.\n",
    "In this context, each mask channel is associated to a different label.\n",
    "The model implemented in the deepskin package, provides the semantic masks as (wound, body, background), but any other order is supported by the following snippet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53b7cf7-9789-4e15-9450-7bd1f4b6c084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we fixed the dimensionality of the input as 256x256\n",
    "IMG_SIZE = 256\n",
    "\n",
    "# define the training parameters for the data loader\n",
    "train_params = {\n",
    "    'target_size':(IMG_SIZE, IMG_SIZE),  # resize shape of the input\n",
    "    'class_mode':'input',  # this is the input of the model\n",
    "    'batch_size':BATCH,    # set the batch size\n",
    "    'shuffle':True,        # enable shuffling of the data\n",
    "    'seed':42,             # fix the random seed for the reproducibility\n",
    "}\n",
    "\n",
    "# define the data loader for the training data (aka images and masks)\n",
    "train_image_generator = image_augmentation.flow_from_directory(\n",
    "    directory=TRAIN_IMAGE_FOLDER,  # set the folder of the images\n",
    "    **train_params,                # set the training parameters\n",
    "    color_mode='rgb',              # the images are in RGB fmt\n",
    "    classes=[''],                  # there are no classes\n",
    "    subset='training'              # this subset is the training one (aka the 90% of the data)\n",
    ")\n",
    "train_masks_generator = masks_augmentation.flow_from_directory(\n",
    "    directory=TRAIN_MASKS_FOLDER,  # set the folder of the masks\n",
    "    **train_params,                # set the training parameters\n",
    "    color_mode='rgb',              # the masks are in multi-channel format\n",
    "    classes=[''],                  # there are no classes\n",
    "    subset='training'              # this subset is the training one (aka the 90% of the data)\n",
    ")\n",
    "\n",
    "# define the data loader for the validation data (aka images and masks)\n",
    "val_image_generator = image_augmentation.flow_from_directory(\n",
    "    directory=TRAIN_IMAGE_FOLDER,  # the validation images belongs to the same folder of the training ones\n",
    "    **train_params,                # set the training parameters\n",
    "    color_mode='rgb',              # the images are in RGB fmt\n",
    "    classes=[''],                  # there are no classes\n",
    "    subset='validation'            # this subset is the validation one (aka the 10% of the data)\n",
    ")\n",
    "val_masks_generator = masks_augmentation.flow_from_directory(\n",
    "    directory=TRAIN_MASKS_FOLDER,  # the validation images belongs to the same folder of the training ones\n",
    "    **train_params,                # set the training parameters\n",
    "    color_mode='rgb',              # the masks are in multi-channel format\n",
    "    classes=[''],                  # there are no classes\n",
    "    subset='validation'            # this subset is the validation one (aka the 10% of the data)\n",
    ")\n",
    "\n",
    "# NOTE: all the data (training and validation) belongs to the same directory\n",
    "# and the internal subdivision is guaranteed by the subset keyword of the\n",
    "# tensorflow function.\n",
    "\n",
    "# Since we want to combine images and masks into a series of pairs, we\n",
    "# can use a pre-processing on the data loader generator to obtain the\n",
    "# correct input for our model\n",
    "\n",
    "from itertools import itemgetter\n",
    "\n",
    "# create training pairs\n",
    "train_generator = zip(map(itemgetter(0), (train_image_generator)), \n",
    "                      map(itemgetter(0), (train_masks_generator))\n",
    "                     )\n",
    "# create validation pairs\n",
    "validation_generator = zip(map(itemgetter(0), (val_image_generator)), \n",
    "                           map(itemgetter(0), (val_masks_generator))\n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6cc233-7d47-418a-a870-1def6ed9968f",
   "metadata": {},
   "source": [
    "Now we can build the model setting the missing training parameters, i.e. the loss function and the optimization strategy.\n",
    "Since the model architecture is already defined in the deepskin package, we can directly import it and setting the training parameters.\n",
    "\n",
    "**NOTE:** Since we are inside an ASSL training round, the model weights **must** be initialized as random at each round!\n",
    "\n",
    "The definition of the loss function and metrics used in the original Deepskin model were combinations of native functions.\n",
    "For their implementation we used the code provided by the `segmentation_models` package (ref. [here](https://github.com/qubvel/segmentation_models)).\n",
    "Importing this library, we defined the loss function as combination of Dice score and Binary Focal Loss functions.\n",
    "The monitoring of the model performances is defined using the standard IoU score and the F-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "248978c6-8378-4dc1-a445-576f6958de24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepskin import deepskin_model\n",
    "import segmentation_models as sm\n",
    "import tensorflow as tf\n",
    "\n",
    "# define the model architecture\n",
    "model = deepskin_model(verbose=False)\n",
    "\n",
    "# define the model optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=1e-5,\n",
    "    beta_1=0.9, beta_2=0.999,\n",
    "    epsilon=1e-7,\n",
    "    amsgrad=False,\n",
    "    name='Adam'\n",
    ")\n",
    "\n",
    "# define the loss function\n",
    "loss = sm.losses.DiceLoss() + (1 * sm.losses.BinaryFocalLoss())\n",
    "\n",
    "# define the metric functions for the model evaluation along\n",
    "# the training epochs\n",
    "iou_score = sm.metrics.IOUScore(threshold=0.5)\n",
    "fscore = sm.metrics.FScore(threshold=0.5)\n",
    "\n",
    "# set the training parameters\n",
    "model.compile(\n",
    "    optimizer=optimizer, \n",
    "    loss=loss,\n",
    "    metrics=[iou_score, fscore],\n",
    "    run_eagerly=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68b4282-0344-407c-8263-d7723aa3d043",
   "metadata": {},
   "source": [
    "When everything about the model parameters is decided and fixed, we can start the training step, enabling all the utilities provided by the Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7b1490-97c6-4c3b-9299-4e9ea2707747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "history = model.fit(\n",
    "    x=train_generator, y=None, # define the input data generators\n",
    "    batch_size=BATCH,          # set the batch size\n",
    "    epochs=100,                # set the maximum number of epochs to perform \n",
    "    steps_per_epoch=train_image_generator.n // BATCH, # define the number of steps for each \n",
    "                                                      # epoch according to the data generator\n",
    "    callbacks=[\n",
    "                                                      # define the callback for the model checkpoint\n",
    "                                                      # setting the output file in which save the best results\n",
    "                                                      # given by the minimum of loss obtained\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            OUT_WEIGHT_FILE, \n",
    "            save_weights_only=True, \n",
    "            save_best_only=True, \n",
    "            mode='min'\n",
    "        ),\n",
    "                                                      # define the callback for the reduction of learning rate\n",
    "                                                      # when a plateau of performances is achieved\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(),\n",
    "                                                      # define the callback for the early stopping of the training\n",
    "                                                      # if there are no improvements in the validation loss for 50\n",
    "                                                      # epochs\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', \n",
    "            min_delta=1e-4, patience=10, \n",
    "            verbose=True,\n",
    "            mode='auto', baseline=None, \n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "    ],\n",
    "    validation_data=validation_generator,             # set the data validation generator\n",
    "    validation_steps=val_image_generator.n // BATCH,  # define the number of steps for each\n",
    "                                                      # validation according to the data generator\n",
    "    initial_epoch=0,                                  # set the initial epoch counter\n",
    "    validation_freq=1,                                # enable the validation at each epoch\n",
    "    max_queue_size=10,                                # queue of data to use\n",
    "    workers=1,                                        # number of threads to use\n",
    "    use_multiprocessing=False,                        # disable multi-processing\n",
    "    shuffle=True,                                     # enable the shuffling of the data at each epoch\n",
    "    verbose=1                                         # set verbosity level of the training\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702973e5-9616-4751-a9a8-637a4cbc2004",
   "metadata": {},
   "source": [
    "At the end of the training the model will achieved the best performances of the current ASSL round.\n",
    "An important step for the monitoring of the performances is the visualization of the obtained results, expressed in terms of metric parameters and loss along the training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77540a7a-0ee1-4120-8e16-74386c3955ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# define the legends for the plots\n",
    "fig1_lbl = [ mpatches.Patch(facecolor='blue', label='Train Loss', edgecolor='k', linewidth=2),\n",
    "             mpatches.Patch(facecolor='orange', label='Val Loss', edgecolor='k', linewidth=2)\n",
    "           ]\n",
    "\n",
    "fig2_lbl = [ mpatches.Patch(facecolor='blue', label='IoU train score', edgecolor='k', linewidth=2),\n",
    "             mpatches.Patch(facecolor='orange', label='IoU val score', edgecolor='k', linewidth=2)\n",
    "           ]\n",
    "\n",
    "fig3_lbl = [ mpatches.Patch(facecolor='blue', label='F1-score train', edgecolor='k', linewidth=2),\n",
    "             mpatches.Patch(facecolor='orange', label='F1-score val', edgecolor='k', linewidth=2)\n",
    "           ]\n",
    "\n",
    "epochs = np.arange(len(history.history['loss']))\n",
    "\n",
    "with sns.plotting_context('paper', font_scale=2):\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3, figsize=(30, 8))\n",
    "    loss = sns.lineplot(x=epochs, y=history.history['loss'],\n",
    "                        markers=True, dashes=False, \n",
    "                        ax=ax1)\n",
    "    val_loss = sns.lineplot(x=epochs, y=history.history['val_loss'],\n",
    "                            markers=True, dashes=False, \n",
    "                            ax=ax1)\n",
    "    ax1.set_ylabel('Mask Loss values')\n",
    "    sns.despine(ax=ax1, offset=10, top=True, right=True, bottom=False, left=False)\n",
    "    ax1.legend(handles=fig1_lbl, loc='upper right')\n",
    "    \n",
    "    \n",
    "    loss = sns.lineplot(x=epochs, y=history.history['iou_score'],\n",
    "                        markers=True, dashes=False, \n",
    "                        ax=ax2)\n",
    "    val_loss = sns.lineplot(x=epochs, y=history.history['val_iou_score'],\n",
    "                            markers=True, dashes=False, \n",
    "                            ax=ax2)\n",
    "    ax2.set_ylabel('IoU loss')\n",
    "    sns.despine(ax=ax2, offset=10, top=True, right=True, bottom=False, left=False)\n",
    "    ax2.legend(handles=fig2_lbl, loc='best')\n",
    "    \n",
    "    loss = sns.lineplot(x=epochs, y=history.history['f1-score'],\n",
    "                        markers=True, dashes=False, \n",
    "                        ax=ax3)\n",
    "    val_loss = sns.lineplot(x=epochs, y=history.history['val_f1-score'],\n",
    "                            markers=True, dashes=False, \n",
    "                            ax=ax3)\n",
    "    ax3.set_ylabel('F1-score loss')\n",
    "    sns.despine(ax=ax3, offset=10, top=True, right=True, bottom=False, left=False)\n",
    "    ax3.legend(handles=fig3_lbl, loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4072df8-a5d7-4a68-88f8-2735aa1fecf9",
   "metadata": {},
   "source": [
    "Now we need to take care about the prediction of the model on the new data, i.e. the data which belongs to the whole dataset.\n",
    "Since we set the restoring of the best model parameters at the end of the training epochs, we can directly apply the model on the new images, sampled on the dataset folder.\n",
    "\n",
    "**NOTE:** since the image needs some pre-processing step, we need to manually apply the required sequence of instruction before inserting it in the model.\n",
    "\n",
    "The correct management of the ASSL training strategy requires the validation of the entire set of images in the dataset at each round of training.\n",
    "Therefore, if you want to check the effectiveness of the ASSL training, the list of files for the model prediction **must** be collected from the `ALL_IMAGE_FOLDER` folder.\n",
    "In contrary, if you want to use the ASSL training strategy just to speed-up your data annotation, you can avoid the re-labeling of the pre-validated images, focusing only on the remaining ones.\n",
    "In the below code, this second option could be enabled un-commenting the first lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee804e4-61aa-4bb4-ba6f-690f8fa21b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "files = glob(f'{ALL_IMAGE_FOLDER}/*')\n",
    "## Uncomment this line for the data labeling feature\n",
    "#files = list(set(files) - set(glob(f'{TRTRAIN_IMAGE_FOLDER}/*')))\n",
    "print(f'{len(files):d} files in the global DB')\n",
    "\n",
    "# get the model input shape\n",
    "  _, h, w, c = model.input.shape\n",
    "\n",
    "# loop along the available images\n",
    "for i, f in enumerate(files):\n",
    "    # log progress\n",
    "    print(\n",
    "        f'\\rFiles {i + 1:d}/{len(files):d}', \n",
    "        flush=True, \n",
    "        end=''\n",
    "    )\n",
    "\n",
    "    # get the image name\n",
    "    name = os.path.basename(f)\n",
    "    # remove the extension to be sure that\n",
    "    # the predicted images will be saved as png\n",
    "    name, _ = os.path.splitext(name)\n",
    "\n",
    "    # load the image\n",
    "    bgr = cv2.imread(f)\n",
    "    # convert the image in RGB fmt\n",
    "    rgb = bgr[..., ::-1]\n",
    "    # resize the image to the \n",
    "    resized = cv2.resize(\n",
    "        rgb,\n",
    "        dsize=(h, w),\n",
    "        interpolation=cv2.INTER_CUBIC\n",
    "    )\n",
    "    # convert the image into floating-point values\n",
    "    resized = np.float32(resized)\n",
    "    # normalize the image into [0, 1] range\n",
    "    resized *= 1. / 255\n",
    "    # extend the dimensionality of the input array\n",
    "    # to the [batch, h, w, c] format\n",
    "    resized = resized.reshape(1, *resized.shape)\n",
    "\n",
    "    # apply the model to get the prediction\n",
    "    pred = model.predict(resized)\n",
    "    # remove useless dimensions from the image\n",
    "    pred = np.squeeze(pred)\n",
    "    # filter the mask output to binary format\n",
    "    pred = np.where(pred > tol, 255, 0)\n",
    "    # convert the mask into uint8 fmt\n",
    "    pred = np.uint8(pred)\n",
    "\n",
    "    # resize the output mask to the same\n",
    "    # shape of the original image, with an\n",
    "    # appropriated interpolation algorithm\n",
    "    pred = cv2.resize(\n",
    "        pred,\n",
    "        dsize=(bgr.shape[1], bgr.shape[0]),\n",
    "        interpolation=cv2.INTER_NEAREST_EXACT\n",
    "    )\n",
    "\n",
    "    # define a canvas on which overlay the predicted mask\n",
    "    # initialized as a copy of the original image\n",
    "    canvas = bgr.copy()\n",
    "    # determine the mask contours for the wound\n",
    "    cnt, _ = cv2.findContours(\n",
    "        pred[..., 0],            # we are assuming that the first channel \n",
    "                                 # will be related to the wound area\n",
    "        cv2.RETR_TREE, \n",
    "        cv2.CHAIN_APPROX_SIMPLE\n",
    "    )\n",
    "    # draw the contours on the canvas\n",
    "    # as gold lines\n",
    "    canvas = cv2.drawContours(\n",
    "        canvas, \n",
    "        cnt, \n",
    "        -1, \n",
    "        (0, 255, 255), # color contours in BGR fmt\n",
    "        3              # linewidth of the contour\n",
    "    )\n",
    "\n",
    "    # save the predicted mask\n",
    "    cv2.imwrite(f'{PRED_MASKS_FOLDER}/{name}.png', pred)\n",
    "    \n",
    "    # save the canvas image for the ASSL validation\n",
    "    cv2.imwrite(f'{PRED_IMAGE_FOLDER}/{name}.png', canvas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a234a888-b561-441b-b791-0bbd0d35c2dd",
   "metadata": {},
   "source": [
    "At the end of this step, we have all the new prediction obtained by this round of ASSL training ready for the manual evaluation by the experts.\n",
    "In the `PRED_MASKS_FOLDER` directory we have the masks generated by the model at this round, while in the `PRED_IMAGE_FOLDER` the list of the original images with the prediction overlayed.\n",
    "Therefore, we are ready for the active learning evaluation which can be easily performed using the `active_learning_validator` scripts available [here](https://github.com/Nico-Curti/active_learning_validator).\n",
    "For the correct usage of the validator, we need to **move** the `PRED_IMAGE_FOLDER` directory in the root of the validator project.\n",
    "Using the web interface we can scroll the list of images, labelling the correctness of the prediction as simple yes-or-no.\n",
    "\n",
    "After the manual validation of the prediction, the `active_learning_validator` software will produce a response file related to this round of ASSL training.\n",
    "In the next code we will assume to have already downloaded the response file and renamed it as `response_round0.csv`.\n",
    "Using the information stored in this file, we will move the correct evaluation to the `TRAIN_MASKS_FOLDER` along the corresponding images.\n",
    "In this way the dataset of available samples will be increased and ready for the next round of ASSL training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b329e73-e5ac-451f-bcdd-f8afa8a058fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "RESPONSE_FILE = f'./response_round{ASSL_ROUND:d}.csv'\n",
    "\n",
    "# load the response outcomes\n",
    "response = pd.read_csv(RESPONSE_FILE, sep=',', header=0)\n",
    "# select only the valid items\n",
    "valid = response.query('response == \"yes\"')\n",
    "# get the corresponding filenames \n",
    "valid = set(valid['Filename'].values)\n",
    "\n",
    "# log the information about this round of validation\n",
    "valid_perc = len(valid)/len(all_files)*100\n",
    "print(f'Correct image validated at round {ASSL_ROUND:d}: {len(valid):d} ({valid_perc:.3f}%)')\n",
    "\n",
    "# evaluate the improvement of the round\n",
    "improvement = len(valid)/len(all_files) - len(round_files)/len(all_files)\n",
    "improvement = '+{:.3f}%'.format(up*100) if up > 0. else '{:.3f}%'.format(up*100)\n",
    "print(f'Improvement obtained at round {ASSL_ROUND:d}: {improvement}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dff9a2-9f97-465b-b79c-db61d64f4637",
   "metadata": {},
   "source": [
    "According to the validate images/masks, we can move the data re-sampling the training dataset for the next round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8651ac83-be73-4af6-9b9f-9f211b553794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# declare the image folder for the next round of ASSL training\n",
    "NEXT_TRAIN_IMAGE_FOLDER = f'./data/deepskin_images_round{ASSL_ROUND + 1:d}'\n",
    "# generate the folder\n",
    "os.makedirs(NEXT_TRAIN_IMAGE_FOLDER, exist_ok=False)\n",
    "\n",
    "# loop along the valid indexes\n",
    "for name in valid:\n",
    "    # build the corresponding filename from the ALL_IMAGE_FOLDER\n",
    "    src = f'{ALL_IMAGE_FOLDER}/{name}'\n",
    "    # create the new destination filename for the copy\n",
    "    dst = f'{NEXT_TRAIN_IMAGE_FOLDER}/{name}'\n",
    "    # copy the file from the whole dataset to the next training set\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "# declare the masks folder for the next round of ASSL training\n",
    "NEXT_TRAIN_MASKS_FOLDER = f'./data/deepskin_masks_round{ASSL_ROUND + 1:d}'\n",
    "# generate the folder\n",
    "os.makedirs(NEXT_TRAIN_MASKS_FOLDER, exist_ok=False)\n",
    "\n",
    "# loop along the valid indexes\n",
    "for name in valid:\n",
    "    # build the corresponding filename from the PRED_MASKS_FOLDER\n",
    "    src = f'{PRED_MASKS_FOLDER}/{name}'\n",
    "    # create the new destination filename for the copy\n",
    "    dst = f'{NEXT_TRAIN_MASKS_FOLDER}/{name}'\n",
    "    # copy the file from the whole dataset to the next training set\n",
    "    shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d884c321-c2dd-4a5f-878b-a99ce0de69d2",
   "metadata": {},
   "source": [
    "At the end of this step, the folder tree should be something like\n",
    "\n",
    "```bash\n",
    "data/\n",
    "├── deepskin_images_round0\n",
    "├── deepskin_images_round1\n",
    "├── deepskin_masks_round0\n",
    "├── deepskin_masks_round1\n",
    "├── deepskin_images\n",
    "├── deepskin_masks\n",
    "├── validation_images_pred_round0\n",
    "└── validation_masks_pred_round0\n",
    "```\n",
    "\n",
    "where:\n",
    "* `deepskin_images` contains the entire set of available images in the dataset;\n",
    "* `deepskin_masks` contains the entire set of **validated** masks in the dataset;\n",
    "* `deepskin_images_round0` contains the images to use during the current (round 0) round of ASSL training;\n",
    "* `deepskin_masks_round0` contains the masks to use during the current (round 0) round of ASSL training;\n",
    "* `deepskin_images_round1` contains the images to use during the **next** (round 1) round of ASSL training;\n",
    "* `deepskin_masks_round1` contains the masks to use during the **next** (round 1) round of ASSL training;\n",
    "* `validation_images_pred_round0` will be filled by the images with the predictions overlayed for the ASSL validation;\n",
    "* `validation_masks_pred_round0` will be filled by the predictions of the trained models.\n",
    "\n",
    "The entire code could be re-run setting the `ASSL_ROUND` as 1 to obtain the next validation step."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
