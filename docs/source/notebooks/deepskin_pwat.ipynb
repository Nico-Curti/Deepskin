{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cae27be4-0831-4b9b-8eec-0c97355e2f6d",
   "metadata": {},
   "source": [
    "# PWAT estimation and prediction\n",
    "\n",
    "In this demo we will show how to perform the prediction of PWAT score using the features extracted from the available set of images.\n",
    "This analysis relies on the availability of a good segmentation model for the wound identification.\n",
    "For the development of the required segmentation model, we refer to the [deepskin_assl](https://github.com/Nico-Curti/blob/main/docs/sources/notebooks/deepskin_assl.ipynb) demo of the `deepskin` package.\n",
    "\n",
    "For the correct application of the code below, we surmise a folder tree close to:\n",
    "\n",
    "```bash\n",
    "data/\n",
    "├── deepskin_images\n",
    "├── deepskin_masks\n",
    "└── pwat_db.csv\n",
    "```\n",
    "\n",
    "where:\n",
    "* `deepskin_images` contains the entire set of available images in the dataset;\n",
    "* `deepskin_masks` contains the entire set of **validated** masks in the dataset;\n",
    "* `pwat_db.csv` file with the PWAT score associated to each image file.\n",
    "\n",
    "In this demo, we will use the functions proposed in `deepskin` package for the extraction of the wound masks and related features, showing how to implement the prediction of the PWAT score using a Lasso regression model.\n",
    "Other customization of the outputs could be obtained adding other features or changes in the machine learning pipeline.\n",
    "\n",
    "First of all we need to load the information about the PWAT scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4ed097-2e21-4f0f-a6bd-b65bdbff3add",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# define the filename of the PWAT labels\n",
    "LABEL_FILENAME = f'./data/pwat_db.csv'\n",
    "\n",
    "# load the file with all the outcomes\n",
    "pwat = pd.read_csv(LABEL_FILENAME, sep=',', header=0)\n",
    "pwat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff429cc5-11c9-4a36-ba49-9ff3374a2c54",
   "metadata": {},
   "source": [
    "The proposed PWAT file **must** have at least two columns, a first refered to the image filename, and a second refered to the associated PWAT score.\n",
    "An example is given by the following table\n",
    "\n",
    "| Filename | PWAT  |\n",
    "|:--------:|:-----:|\n",
    "| image0.png |  12 |\n",
    "| image1.png |  4  |\n",
    "| ...        | ... |\n",
    "| imageN.png | 17  |\n",
    "\n",
    "According to the available images/masks, we will apply the `deepskin` pipeline for the extraction of the image features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f7acb0-716a-46cb-9d79-e22bbdf005d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "# define the filename of the PWAT labels\n",
    "# define the directory in which the whole DB of images are stored\n",
    "ALL_IMAGE_FOLDER = './data/deepskin_images'\n",
    "# define the directory in which the whole DB of (validated!!) masks are stored\n",
    "ALL_MASKS_FOLDER = './data/deepskin_masks'\n",
    "\n",
    "# get all the image filenames\n",
    "image_files = glob(f'{ALL_IMAGE_FOLDER}/*')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf3bd11-7ab9-4bbd-bb41-cdb1cc4748d5",
   "metadata": {},
   "source": [
    "Now using the list of available images, we apply the feature extraction procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1642336c-cf18-4222-89b8-322d812a3d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from deepskin import evaluate_features\n",
    "\n",
    "features = {}\n",
    "\n",
    "# loop along the available image files:\n",
    "for i, image_file in enumerate(image_files):\n",
    "\n",
    "    # get the filename\n",
    "    name = os.path.basename(image_file)\n",
    "    \n",
    "    # build the associated mask filename\n",
    "    mask_file = f'{ALL_MASKS_FOLDER}/{name}'\n",
    "    \n",
    "    # load the image using opencv\n",
    "    bgr = cv2.imread(image_file)\n",
    "    # convert the image from BGR to RGB fmt\n",
    "    rgb = bgr[..., ::-1]\n",
    "\n",
    "    # load the mask using opencv\n",
    "    mask = cv2.imread(mask_file, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    # compute the features related to the current file\n",
    "    features[name] = evaluate_features(\n",
    "        img=rgb,\n",
    "        mask=mask\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46041450-9aa8-4926-8b17-c6d3e0224d5b",
   "metadata": {},
   "source": [
    "At the end of this step, you will have a set of numeric features which characterize both the wound and peri-wound areas identified by the segmentation model.\n",
    "A detailed description of the features pre-computed by the `deepskin` model is discussed in the work of Curti et al.\n",
    "\n",
    "For a faster evaluation, a re-organization of the features into a dataframe structure could help for the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ae072a-68eb-40e0-85ea-3a45c7d0d53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.DataFrame.from_dict(features, orient='index').T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5163ae-806a-4d91-b07d-5c00a10c34c0",
   "metadata": {},
   "source": [
    "Now we can start to build the prediction pipeline composed by a standardization step and a regression model.\n",
    "We will use the scikit-learn package to build our pipeline, splitting the available data into a K-fold cross-validation.\n",
    "The use of cross-validation ensures the correct evaluation of the model performances and the testing of the results into independent set of values.\n",
    "However, at the end of the cross validation we will have K different regression model, each one tested on a subset of the available data.\n",
    "Thus, the use of the cross-validation is used to check the effectiveness of the proposed pipeline for the PWAT evaluation, while for the final deploy of the model we will train a final model using all the available data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3b1da9-a694-475a-9509-253b5b75d8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler # standardization algorithm\n",
    "from sklearn.model_selection import StratifiedKFold # cross validation algorithm\n",
    "from sklearn.linear_model import Lasso # regression model\n",
    "from sklearn.pipeline import make_pipeline # whole pipeline workflow manager\n",
    "from sklearn.model_selection import cross_val_predict # pipeline workflow\n",
    "from scipy.stats import pearsonr, spearmanr # model evaluation metrics\n",
    "\n",
    "# define the K-fold algorithm setting the K value\n",
    "K = 10\n",
    "# use a stratified K-Fold to ensure a good subdivision of\n",
    "# the data among train and test\n",
    "cv10 = StratifiedKFold(\n",
    "    n_splits=K,     # set the value of K\n",
    "    shuffle=True,   # enable the shuffling of the data\n",
    "    random_state=42 # fix the random seed for results reproducibility\n",
    ")\n",
    "\n",
    "# define the standardization method\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# define the regression model\n",
    "regressor = Lasso(\n",
    "    alpha=1e-2, \n",
    "    fit_intercept=True, \n",
    "    random_state=42, \n",
    ")\n",
    "\n",
    "# define the pipeline steps\n",
    "pipeline_steps = [scaler, regressor]\n",
    "# build the machine learning pipeline\n",
    "pipe = make_pipeline(*pipeline_steps)\n",
    "\n",
    "# define the data (X) and labels (y_true) on which apply the pipeline\n",
    "X = features\n",
    "y_true = pwat['PWAT']\n",
    "\n",
    "# run the pipeline to get the prediction\n",
    "y_pred = cross_val_predict(pipe, X, y_true, cv=cv10)\n",
    "\n",
    "# evaluate the pipeline performances\n",
    "stat_pearson = pearsonr(y_true, y_pred)\n",
    "stat_spearman = spearmanr(y_true, y_pred)\n",
    "\n",
    "print(f\"Pearson's R score: {stat_pearson[0]:.3f} (p-value: {stat_pearson[1]:.3f})\")\n",
    "print(f\"Spearman's R score: {stat_spearman[0]:.3f} (p-value: {stat_spearman[1]:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729cc230-7d78-469d-9f75-8ab48b3d8e5c",
   "metadata": {},
   "source": [
    "According to the performances obtained by the model, we can test the robustness of the developed pipeline re-iterating the cross-validation process on different train/test subdivisions, collecting the resulting metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7cb820-eaaf-4c1d-bd68-9fd96187b63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 100 CV to test the robustness\n",
    "spearman_coefs = []\n",
    "\n",
    "# generate 100 iterations\n",
    "for i in range(100):\n",
    "    # define a different K-fold changing the random seed\n",
    "    cv10 = StratifiedKFold(n_splits=K, shuffle=True, random_state=i+1)\n",
    "    # run the pipeline\n",
    "    y_pred = cross_val_predict(pipe, X, y_true, cv=cv10)\n",
    "    # evaluate the metrics\n",
    "    stat_spearman = spearmanr(y_true, y_pred)\n",
    "    spearman_coefs.append(stat_spearman)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
